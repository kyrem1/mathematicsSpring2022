\documentclass[12pt,letterpaper]{article}

%--------Packages--------
\usepackage{amsmath, amsthm, amssymb}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{array}
\usepackage{braket}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{delarray}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{faktor} % For quotients
\usepackage{mathrsfs}

% \usepackage{quiver}
\usepackage[linguistics]{forest}




%--------Page Setup--------

\pagestyle{empty}%

\setlength{\hoffset}{-1.54cm}
\setlength{\voffset}{-1.54cm}

\setlength{\topmargin}{0pt}
\setlength{\headsep}{0pt}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0pt}

\setlength{\textwidth}{195mm}
\setlength{\textheight}{250mm}


%--------Macros--------

\newcommand{\sub}{\subseteq}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\sO}{\mathcal{O}}
\newcommand{\cyclic}[1]{\langle#1\rangle}
\newcommand{\units}[1]{#1 ^{\times}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
%----Switch phi and varphi
\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}\xspace}
\newcommand{\I}{\mathbb{I}\xspace}
\newcommand{\R}{\mathbb{R}\xspace}
\newcommand{\Z}{\mathbb{Z}\xspace}
\newcommand{\Q}{\mathbb{Q}\xspace}
\newcommand{\G}{\mathbb{G}\xspace}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\dv}{div}
\DeclareMathOperator{\alb}{alb}
\DeclareMathOperator{\img}{Im}
\DeclareMathOperator{\et}{et}
\DeclareMathOperator{\ck}{coker}
\DeclareMathOperator{\Reg}{Reg}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Ac}{at}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Pic}{Pic}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\fc}{frac}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Mod}{Mod}
\DeclareMathOperator{\Cone}{Cone}
\DeclareMathOperator{\FI}{FI}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Alb}{Alb}
\DeclareMathOperator{\Ext}{Ext}
\DeclareMathOperator{\ab}{ab}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\fr}{frac}
\DeclareMathOperator{\spn}{span}


%----Analysis
\newcommand{\dd}[2][]{\frac{\partial^{#1}}{\partial {#2}^{#1}}}
\newcommand{\summ}{\sum\limits}
\newcommand{\norm}[1]{\left \vert \left \vert #1 \right \vert \right \vert}
\newcommand{\thicc}{\bigg}
\newcommand{\eps}{\varepsilon}


%--------Theorem environments--------
\newtheorem{definition}{Definition}[]
\newtheorem{lemma}{Lemma}[]
\newtheorem{corollary}{Corollary}[]
\newtheorem{theorem}{Theorem}[]
\theoremstyle{remark}
\newtheorem*{claim}{Claim}


\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}


\makeatletter
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother

% --------Problem environment--------
\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}


\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \stepcounter{homeworkProblemCounter}
}


%--------Metadata--------
\title{MATH 7752 Homework 2}
\author{James Harbour}



\begin{document}
\maketitle

\begin{homeworkProblem}
  Let $D$ be a division ring (not necessarily commutative) and $M$ be a $D$-module.\\

  \textbf{(a)} Let $X$ be a generating set of $M$ and $Y$ a $D$-linearly independent subset of $X$. Prove that $M$ has a $D$-basis $B$ with $Y\sub B\sub X$.

  \begin{proof}
    Consider the poset $\ms{S} = \{ B\sub M : Y\sub B\sub X \text{ and } B \text{ is $D$-linearly independent}\}$ ordered by inclusion. Since $Y$ is a $D$-linearly independent subset of $X$, we have that $Y\in \ms{S}$ so $\ms{S}\neq\emptyset$. \\

    Suppose that $\ms{C}\sub\ms{S}$ is any linearly ordered chain in $\ms{S}$. Let $B = \bigcup \ms{C}$. Then $Y\sub B\sub X$. Suppose that $d_i\in D$ and $b_i\in B$ such that $\sum_{i=1}^{n} d_i \cdot b_i = 0$. Then for each $i\in \{ 1, \ldots, n\}$, there exists a $B_i\in \ms{C}$ such that $b_i\in B_i$. As $\ms{C}$ is a chain, there is some $l\in \{ 1,\ldots, n\}$ such that $B_i\sub B_l$ for all $1\leq i \leq n$. It follows that $b_i\in B_l$ for all $1\leq i\leq n$, whence $B_l$ being $D$-linearly independent implies that $d_i = 0$ for all $i$. Thus $B$ is $D$-linearly independent, so $B\in \ms{S}$. \\

    Now by Zorn's lemma, there exists a maximal element $B\in \ms{S}$ of $\ms{S}$. We claim that $B$ is in fact a $D$-basis for $M$. It suffices to show that $B$ is a generating set for $M$. Let $N = \spn_D(B)$. Suppose, for the sake of contradiction, that $N \neq M$. As $B\sub X$ and $X$ is a generating set for $M$, it follows that there exists an $x\in X\setminus \spn_D(B)$. Suppose $r, r_1, \ldots, r_n\in R$ are such that
    \[
      0 = rx + r_1 b_1 + \cdots + r_n b_n.
    \]
    If $r\neq 0$, then
    \[
      x = (-r^{-1} r_1)\cdot b_1 + \cdots + (-r^{-1} r_1)\cdot r_n,
    \]
    which would imply that $x\in \spn_D(B)$, contradicting the choice of $x$. Hence $r=0$, so $B$ being $D$-linearly independent implies that $r_i = 0$ for all $i$. Thus $B\cup \{x\}$ is $D$-linearly independent, contradicting the maximality of $B$.
  \end{proof}

  \textbf{(b)} Conclude that every non zero $D$-module $M$ has a $D$-basis.

  \begin{proof}
    Since $M\neq 0$, there exists an $y\in M\setminus \{ 0\}$. It follows that the singleton $\{ y\}$ is a $D$-linearly independent subset of $M$. On the other hand, $M = 1\cdot M$, so the set $M$ is a generating set of $M$. Applying part (a) to $X = M$ and $Y = \{ y\}$, it follows that $M$ has a $D$-basis.
  \end{proof}
\end{homeworkProblem}

\begin{homeworkProblem}
    Let $R$ be a commutative domain. Let $I$ be a non-principal ideal of $R$. SHow that when $I$ is considered as an $R$-module (by left multiplication), then $I$ is indecomposable but not cyclic.

    \begin{proof}
      Since $I$ is non-principal, by definition $I$ is not cyclic as an $R$-module. Suppose, for the sake of contradiction, that $I=P\oplus Q$ for some nonzero proper $R$-submodules $P,Q$ of $I$. Take $p\in P\setminus \{ 0\}$ and $q\in Q\setminus\{ 0\}$. Then $p\cdot q - q\cdot p = pq-qp = 0 \implies p\cdot q= q\cdot p$. As $R$ is a domain $pq=qp \neq 0$, whence $pq=qp\in P\cap Q$ contradicts that the sum $P\oplus Q$ is direct.
    \end{proof}
\end{homeworkProblem}

\begin{homeworkProblem}
  Let $R$ be a commutative ring. An $R$-module $M$ is called \emph{torsion} if for any $m\in M$ there exists some nonzero $r\in R$ such that $rm=0$. An $R$-module $N$ is called \emph{divisible} if for any nonzero $r\in R$ it holds that $rN=N$. \\

  \textbf{(a)} Suppose $M$ is a torsion $R$-module and $N$ is a divisible $R$-module. Prove that $M\otimes_R N = \{ 0\}$.

  \begin{proof}
    Let $m\in M$ and $n\in N$. Since $M$ is torsion, there exists a nonzero $r \in R$ such that $rm=0$. Now, by divisibility of $N$, there exists an $n'\in N$ such that $rn' = n$. Hence
    \[
      m\otimes n = m \otimes rn' = rm \otimes n' = 0\otimes n' = 0.
    \]
    Thus every simple tensor in $M \otimes_R N$ is $0$, whence $M \otimes_R N = 0$.
  \end{proof}

  \textbf{(b)} Consider the $\Z$-module $M = \Q/\Z$. Prove that $M\otimes_{\Z} M = \{ 0\}$

  \begin{proof}
    We show that $M$ is both torsion and divisible. Note that for any $\frac{p}{q} + \Z\in \Q/\Z$, $q\neq 0$ and $q\cdot (\frac{p}{q} + \Z) = q\cdot\frac{p}{q} + \Z = p + \Z = \Z$, so $\Q/\Z$ is torsion. \\

    On the other hand, suppose $n\in \Z\setminus \{ 0\}$. For $\frac{p}{q} + \Z \in \Q/\Z$, observe that
    \[
      n\cdot\left(\frac{p}{nq} + \Z \right) = \frac{np}{nq} + \Z = \frac{p}{q} + \Z
    \]
    so $\Q/\Z$ is divisible. Appealing to part(a), it follows that $M\otimes_{\Z} M = 0$.
  \end{proof}
\end{homeworkProblem}

\begin{homeworkProblem}
  Let $R$ be a PID and $A$ be an $R$-module. Let $K$ be the field of fractions of $R$, and consider the $K$-module $B = K\otimes_R A$. Prove that every $z\in B$ is a simple tensor.

  \begin{proof}
    Let $z\in B$. Then there exists $\frac{x_1}{s_1}, \ldots,\frac{x_n}{s_n} \in K$, $a_1, \ldots, a_n\in A$ and $c_1, \ldots, c_n \in R$ such that
    \[
      z = \sum_{i=1}^{n} c_i \cdot\left(\frac{x_i}{s_i} \otimes a_i\right) = \sum_{i=1}^{n} \frac{c_i x_i}{s_i} \otimes a_i = \sum_{i=1}^{n} \frac{c_i x_i \prod_{j\neq i}s_j}{s_1 \cdots s_n} \otimes a_i.
    \]
    Since $R$ is a PID, there exists an $s\in R$ such that $(s) = \left\la c_i x_i \prod_{j\neq i} s_j: 1\leq i\leq n \right\ra$. Then for each $i\in \{ 1,\ldots, n\}$, there is an $r_i\in R$ such that $c_i x_i \prod_{j\neq i} s_j = r_i s$. Hence,
    \[
      z = \sum_{i=1}^{n} \frac{c_i x_i \prod_{j\neq i}s_j}{s_1 \cdots s_n} \otimes a_i = \sum_{i=1}^{n} \frac{r_i s}{s_1 \cdots s_n} \otimes a_i = \sum_{i=1}^{n} \frac{ s}{s_1 \cdots s_n} \otimes r_i a_i = \frac{ s}{s_1 \cdots s_n} \otimes \left(\sum_{i=1}^{n}r_i a_i\right)
    \]
    is a simple tensor.
  \end{proof}
\end{homeworkProblem}

\begin{homeworkProblem}
  Let $R$ be a commutative ring and $M$ an $R$-module. \\

  \textbf{(a)}: Let $I$ be an ideal of $R$. Prove an isomorphism
  \[
    R/I \otimes M \simeq M/IM.
  \]

  \begin{proof}
    % Consider the exact sequence
    % \[
    %   0\to I\rightarrow R \rightarrow R/I \to 0.
    % \]
    % Upon applying the right exact functor $-\otimes_R M$ to the above exact sequence, we obtain the exact sequence
    % \[
    %   I\otimes_R M \rightarrow R\otimes_R M\cong M \rightarrow R/I \otimes_R M \to 0
    % \]
    % The image of $I\otimes_R M$ under the identification $R\otimes_R M \cong M$ via $r\otimes m \mapsto r\cdot m$ is $IM$, hence by the first isomorphism theorem $M/IM \cong R/I \otimes_R M$.

    On one hand, consider the map $\widetilde\Psi: M \to R/I\otimes_R M$ given by $\widetilde\Psi(m):= (1+I)\otimes m$ for $m\in M$. This map is clearly an $R$-module homomorphism by the second and third defining relations the tensor product. For $i\in I$ and $m\in M$, $\widetilde\Psi(im) = (1+I)\otimes im = i\cdot(1 + I) \otimes m = 0$, so the generators of $IM$ lie in $\ker(\widetilde\Psi)$ whence $IM\sub \ker(\widetilde\Psi)$. Hence, $\widetilde\Psi$ descends to an $R$-module homomorphism $\Psi: M/IM \to R/I\otimes_R M$ such that $\Psi(m+IM) = \widetilde\Psi(m)$. Observe that, for $r\in R$ and $m\in M$, $\Psi(rm+IM) = (1+I)\otimes r\cdot m = (r+I)\otimes m$, so $\Psi(M/IM)$ contains all simple tensors, whence by linearity $\Psi$ is surjective. \\

    On the hand, consider the map $\widetilde\Phi: R/I \times M \to M/IM$ given by $\widetilde\Phi(r+I, m) = rm + IM$. To see that this is well-defined, if $r+I = r'+I\in R/I$, then $r-r'\in I$ whence $(r-r')\cdot m + IM = IM$. Suppose now that $m, n\in M$, $r+I,s+I\in R/I$, and $x\in R$. Then
    \begin{align*}
      \widetilde\Phi((r+I)+(s+I),m) &= \widetilde\Phi((r+s)+I,m) = (r+s)\cdot m+IM \\ &= (r\cdot m + IM) + (s \cdot m + IM) = \widetilde\Phi(r+I,m) + \widetilde\Phi(s+I,m) \\
      \widetilde\Phi(r+I,m+n) &= r\cdot(m+n) + IM = r\cdot m + r\cdot n + IM = (r\cdot m + IM) + (r\cdot n + IM) \\ &= \widetilde\Phi(r+I,m) + \widetilde\Phi(r+I,n) \\
      \widetilde\Phi(x\cdot (r+I),m) &= \widetilde\Phi(xr+I,m) = (xr)\cdot m + IM = x\cdot(r\cdot m+IM) = x\cdot\widetilde\Phi(r+I,m) \\
      \widetilde\Phi(r+I, x\cdot m) &= r\cdot (x\cdot m) + IM = x\cdot (r\cdot m + IM) = x\cdot\widetilde\Phi(r+I,m),
    \end{align*}
    so $\widetilde\Phi$ is an $R$-bilinear map. By the universal property of tensor products, there exists a unique $R$-module homomorphism $\Phi:R/I\otimes M\to M/IM$ such that $\Phi((r+I)\otimes m) = \widetilde\Phi(r+I, m)$ for all $r\in R$ and $m\in M$. \\

    Now we show that $\Phi$ and $\Psi$ are mutual inverses. On one hand, for $m+IM \in M/IM$,
    \[
      \Phi(\Psi(m+IM)) = \Phi((1+I)\otimes m) = 1\cdot m + IM = m + IM,
    \]
    so $\Phi\circ\Psi= id_{M/IM}$. For the other direction, it suffices by linearity to prove that $\Psi\circ\Phi$ is the identity on just the simple tensors. For $(r+I)\otimes m \in R/I\otimes M$,
    \[
      \Psi(\Phi((r+I)\otimes m)) = \Psi(rm + IM) = (1+I)\otimes rm = (r+I)\otimes m
    \]
    so $\Psi\circ\Phi$ is the identity on simple tensors, whence $\Psi\circ\Phi = id_{R/I\otimes M}$.

  \end{proof}

  \textbf{(b)}: Suppose that $M$ is a finitely generated free $R$-module. Show that the \emph{rank} of $M$ is well-defined, i.e. any two $R$-bases of $M$ have the same number of elements.

  \begin{proof}
    Let $\mf{m}\sub R$ be a maximal ideal of R. Let $k=R/\mf{m}$ be the corresponding residue field. Suppose that $n,l\in \N$ such that $R^l \cong M \cong R^n$. Then,

    \[
      k^l \cong (R/\mf{m}\otimes R)^l \cong R/\mf{m}\otimes R^l \cong R/\mf{m} \otimes R^n \cong (R/\mf{m}\otimes R)^n \cong k^n
    \]

    as $R$-modules. Let $\phi:k^l\to k^n$ be the composition of the above $R$-module isomorphisms. Note that $\mf{m}\sub \Ann_R(k^l), \Ann_R(k^n)$, so the $k$-module structures on $k^l, k^m$ given by $(r+\mf{m})\cdot a := r\cdot a$ and $(r+\mf{m})\cdot b$ for $a\in k^l$ and $b\in k^n$ are well-defined. Moreover, for $r\in R$ and $a\in k^l$,
    \[
      \phi((r+\mf{m})\cdot a) = \phi(r\cdot a) = r\cdot \phi(a)  = (r+\mf{m})\cdot\phi(a),
    \]
    so $\phi$ is also a $k$-module isomorphism. As $k^l$ and $k^n$ are isomorphic $k$-vector spaces, it follows that $l=n$.

  \end{proof}
\end{homeworkProblem}

\begin{homeworkProblem}
  Let $R\sub S$ be an inclusion of commutative rings. Consider the polynomial rings $R[x]$ and $S[x]$. Prove that there is an isomorphism of $S$-modules,
  \[
    S\otimes_R R[x]\rightarrow S[x].
  \]

  \begin{proof}
    On one hand, there is an obvious map $\Psi:S[x]\to S\otimes_R R[x]$ defined by
    \[
      \Psi\left(\sum_{k=0}^{n} s_k x^k\right) := \sum_{k=0}^{n}s_k\otimes x^k.
    \]
    Then, for monomials $s_k x^k, t_k x^k \in S[x]$ and $s\in S$,
    \[
      \Psi(s\cdot(s_k x^k) + t_k x^k) = \Psi((ss_k +t_k)x^k) = (ss_k+t_k)\otimes x^k = s\cdot (s_k\otimes x^k) + t_k\otimes x^k = s\cdot\Psi(s_k x^k) + \Psi(t_k x^k),
    \]
    whence via extending linearly and applying this relation, it follows that $\Psi$ is an $S$-module homomorphism. \\

    On the other hand, consider the map $\widetilde\Phi:S\times R[x]\to S[x]$ given by $(s,f(x))\mapsto sf(x)$. This map is clearly $R$-bilinear as it is a multiplication map, so there exists a unique $R$-module homomorphism $\Phi:S\otimes R[x]\to S[x]$ such $\Phi(s\otimes f(x)) = \widetilde\Phi(s,f(x)) = sf(x)$. Moreover, we will show that this map is in fact an $S$-module homomorphism. It suffices to show this relation on simple tensors, whence linearity would imply that it holds on all of $S\otimes_R R[x]$ considered as an $S$-module. Let $s,s'\in S$ and $f(x)\in R[x]$. Then,
    \[
      \Phi(s \cdot (s'\otimes f(x))) = \Phi((ss')\otimes f(x)) = ss'f(x) = s\cdot\Phi(s'\otimes f(x)).
    \]

    Now we show that the $S$-module homomorphisms $\Psi$ and $\Phi$ are mutual inverses. On one hand, suppose that $f(x) = \sum_{k=0}^{n} s_k x^k\in S[x] $. Then
    \[
      \Phi(\Psi(f(x))) = \Phi\left(\sum_{k=0}^{n}s_k\otimes x^k\right) = \sum_{k=0}^{n}\Phi(s_k\otimes x^k ) = \sum_{k=0}^{n} s_k x^k = f(x),
    \]
    so $\Phi\circ\Psi = id_{S[x]}$. On the other hand, it suffices to show that $\Psi\circ\Phi$ agrees with the identity on simple tensors, whence by linearity it would agree with the identity on all of $S\otimes_R R[x]$. Suppose $f(x) = \sum_{k=0}^{n} r_k x^k\in R[x]$ and $s\in S$. Then,
    \[
      \Psi(\Phi(s\otimes f(x))) = \Psi(sf(x)) = \sum_{k=0}^{n} \Psi(sr_k x^k) = \sum_{k=0}^{n} (sr_k)\otimes x^k = \sum_{k=0}^{n} s\otimes r_k x^k = s\otimes\sum_{k=0}^{n}  r_k x^k = s\otimes f(x),
    \]
    so $\Psi\circ\Phi = id_{S\otimes R[x]}$.
  \end{proof}
\end{homeworkProblem}

\begin{homeworkProblem}
  Let $R$ be a commutative ring and $I_1, \ldots, I_k$ be a finite collection of ideals of $R$. Let $M$ be an $R$-module.\\

  \textbf{(a)} Prove that the map $f:M\to \frac{M}{I_1 M}\times \cdots\times \frac{M}{I_k M}$ defined by \[m\mapsto (m+I_1 M, \ldots, m+I_k M)\] is an $R$-module homomorphism with kernel $I_1 M\cap \cdots \cap I_k M$.

  \begin{proof}
    Let $m,n\in M$ and $r\in R$. Then
    \[
      f(r\cdot m+n) = (r\cdot m + n + I_1 M, \ldots, r\cdot m + n + I_k M) =  r\cdot(m + I_1 M, \ldots, m + I_k M) + (n + I_1 M, \ldots, n + I_k M) = r\cdot f(m) + f(n),
    \]
    so $f$ is an $R$-module homomorphism. It is clear that $I_1 M\cap \cdots \cap I_k M \sub \ker(f)$, so it remains to show the reverse inclusion. Suppose that $m\in\ker(f)$. Then $(I_1 M, \ldots, I_k M) = f(m) = (m+I_1 M, \ldots, m+I_k M)$, whence $m\in I_j M$ for $1\leq j\leq k$, so $m\in I_1 M\cap \cdots \cap I_k M$.
  \end{proof}

  \textbf{(b)} Assume in addition that the ideals $I_1,\ldots, I_k$ are pairwise comaximal. Show that there is an isomorphism of $R$-modules,
  \[
    \frac{M}{(I_1\cdots I_k)M}\cong \frac{M}{I_1 M}\times\cdots\times \frac{M}{I_k M}.
  \]
  \begin{proof}
    We proceed by induction on the integer $k\geq 2$. Suppose first that $k=2$. Then by comaximality, there exists an $r\in I_1$ and $s\in I_2$ such that $r+s=1$. Then $1-s = r\in I_1$, so for $m\in M$,
    \begin{align*}
      f(rm) &= (rm+I_1 M, rm+I_2 M) = (I_1 M, (1-s)\cdot m +I_2 M) = (I_1 M, m +I_2 M) \\
      f(sm) &= (sm+I_1 M, sm+I_2 M) = ((1-r)\cdot m+I_1 M, I_2 M) = (m+I_1 M, I_2 M).
    \end{align*}
    Hence, for a fixed $(m+I_1 M,n+I_2 M)\in M/I_1 M\times M/I_2 M$,
    \[
      f(rn + sm) = f(rn) + f(sm) = (m + I_1 M, n + I_2 M),
    \]
    so $f$ is surjective. Now we show that $I_1\cap I_2 = I_1 I_2$. The reverse inclusion is true \emph{a priori}, regardless of comaximality of the ideals. For the forward inclusion, suppose that $x\in I_1\cap I_2$. Then $x= x(r+s) = xr+xs \in I_1 I_2$. Hence $\ker(f)= I_1 I_2$ by part (a), so the claim follows via the first isomorphism theorem. \\

    Now fix $k>2$ suppose that the claim holds for all integers less than $k$. Consider the ideals $I = I_1$ and $J = I_2 \cdots I_k$. We claim that these ideals are comaximal. For $j\in \{ 2,\ldots, k\}$, choose $r_j\in I$ and $s_j\in I_i$ such that $r_j + s_j = 1$. Then, every term in the expansion of the product $1 = (x_2+y_2)\cdots (x_k+y_k)$ is in $I$ except for the term $y_2\cdots y_k\in J$, so $1 \in I+J$ i.e. $I,J$ are comaximal. By the induction hypothesis,
    \[
      \frac{M}{(I_1\cdots I_k)M}= \frac{M}{(IJ) M} \cong \frac{M}{I_1 M}\times \frac{M}{(I_2\cdots I_k) M} \cong \frac{M}{I_1 M} \times\cdots\times \frac{M}{I_k M}.
    \]
  \end{proof}
\end{homeworkProblem}

\end{document}
