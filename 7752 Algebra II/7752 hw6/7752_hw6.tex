\documentclass[12pt,letterpaper]{article}

%--------Packages--------
\usepackage{amsmath, amsthm, amssymb}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{array}
\usepackage{braket}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{delarray}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{faktor} % For quotients
\usepackage{mathrsfs}
% \usepackage{quiver}
% \usepackage{tikz}

% \usepackage{quiver}
\usepackage[linguistics]{forest}




%--------Page Setup--------

\pagestyle{empty}%

\setlength{\hoffset}{-1.54cm}
\setlength{\voffset}{-1.54cm}

\setlength{\topmargin}{0pt}
\setlength{\headsep}{0pt}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0pt}

\setlength{\textwidth}{195mm}
\setlength{\textheight}{250mm}


%--------Macros--------

\newcommand{\ilm}[1]{\begin{psmallmatrix} #1 \end{psmallmatrix}}
\newcommand{\ilmb}[1]{\boxed{\begin{smallmatrix} #1 \end{smallmatrix}}}

\newcommand{\sub}{\subseteq}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\sO}{\mathcal{O}}
\newcommand{\cyclic}[1]{\langle#1\rangle}
\newcommand{\units}[1]{#1 ^{\times}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\lr}[1]{\left(#1\right)}
%----Switch phi and varphi
\let\temp\phi
\let\phi\varphi
\let\varphi\temp

\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}\xspace}
\newcommand{\I}{\mathbb{I}\xspace}
\newcommand{\R}{\mathbb{R}\xspace}
\newcommand{\Z}{\mathbb{Z}\xspace}
\newcommand{\Q}{\mathbb{Q}\xspace}
\newcommand{\G}{\mathbb{G}\xspace}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\dv}{div}
\DeclareMathOperator{\alb}{alb}
\let\Im\relax
\DeclareMathOperator{\Im}{Im}
\DeclareMathOperator{\et}{et}
\DeclareMathOperator{\ck}{coker}
\DeclareMathOperator{\Reg}{Reg}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Ac}{at}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Pic}{Pic}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\fc}{frac}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Mod}{Mod}
\DeclareMathOperator{\Cone}{Cone}
\DeclareMathOperator{\FI}{FI}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Alb}{Alb}
\DeclareMathOperator{\Ext}{Ext}
\DeclareMathOperator{\ab}{ab}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\fr}{frac}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\GL}{GL}


%----Analysis
\newcommand{\dd}[2][]{\frac{\partial^{#1}}{\partial {#2}^{#1}}}
\newcommand{\summ}{\sum\limits}
\newcommand{\norm}[1]{\left \vert \left \vert #1 \right \vert \right \vert}
\newcommand{\thicc}{\bigg}
\newcommand{\eps}{\varepsilon}
\newcommand*\cls[1]{\overline{#1}}


%--------Theorem environments--------
\newtheorem{definition}{Definition}[]
\newtheorem{lemma}{Lemma}[]
\newtheorem{corollary}{Corollary}[]
\newtheorem{theorem}{Theorem}[]
\theoremstyle{remark}
\newtheorem*{claim}{Claim}


\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}


\makeatletter
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother

% --------Problem environment--------
\setlength\parindent{0pt}
\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}


\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \stepcounter{homeworkProblemCounter}
}


%--------Metadata--------
\title{MATH 7752 Homework 6}
\author{James Harbour}


\begin{document}
\maketitle


\begin{homeworkProblem}
  \textbf{(a)} Prove that two $3\times 3$ matrices over some field $F$ are similar if and only if they have the same minimal and characteristic polynomials. Is the same true for $4\times 4$ matrices?

  \begin{proof}
    The forward direction is clear and true for $n\times n$ matrices, so suppose that $A,B\in M_3(F)$ such that $\mu_A=\mu_B=\mu$ and $\chi_A=\chi_B=\chi$. Let $\alpha_1
    |\cdots|\alpha_m$ and $\beta_1|\cdots|\beta_n$ be the invariant factors for $A$ and $B$ respectively. As $\sum\deg(\alpha_i) = 3$ and $\sum\deg(\beta_i) = 3$, it follows that $m,n\leq 3$. To show that $A$ and $B$ are similar, it suffices to show that their invariant factors are the same as then their RCFs would be the same.\\

    Suppose, without loss of generality, that $m\geq n$. If $m=n=1$, then $\alpha_1 = \chi = \beta_1$. If $n=1$ and $m > 1$, then \[\alpha_1\cdots \alpha_{m-1}\mu = \chi = \beta_1 = \mu \implies \alpha_1\cdots \alpha_{m-1} = 1\] contradicting that the invariant factors are nonunits. \\

    If $m=n=2$, then $\alpha_1\mu = \beta_1\mu \implies \alpha_1 = \beta_1$. \\

    If $m=3$ and $n=2$, then all the $\alpha_i$'s are degree one monic whence successive divisibility forces them to be equal, say $\alpha_1 = \alpha_2=\alpha_3=\alpha$ with $\alpha$ monic of degree 1. But then $\alpha^2 = \alpha_1 \alpha_2 = \beta_1$, whence $\beta_1$ is degree 2 and $\beta_2$ is degree at least 2, contradicting that their degrees sum to 3.\\

    If $m=n=3$, then each of the $\alpha_i$'s are equal to some linear $\alpha$ and each of the $\beta_i$'s are equal to some linear $\beta$. Moreover, $\alpha= \alpha_3 = \mu = \beta_3 = \beta$, so all of the invariant factors are the same.\\

    The same is not true for $4\times 4$ matrices. Consider the matrices
    \[A=\begin{pmatrix}1&1& &\\&1& &\\&&1&1\\&&&1 \end{pmatrix},\hspace{20pt} B=\begin{pmatrix}1&&&\\&1& &\\&&1&1\\&&&1 \end{pmatrix}\]
    Then $\chi_A = (x-1)^4 = \chi_B$ and $\mu_A = (x-2)^2 = \mu_B$; however, $A$ and $B$ are already in RCF and not equal, so they are not similar.
  \end{proof}

  \textbf{(b)} A matrix $A$ is called idempotent if $A^2=A$. Prove that two idempotent $n\times n$ matrices are similar if and only if they have the same rank. \textbf{Hint:} What is the minimal polynomial of an idempotent matrix? How does rank relate to eigenvalue $0$?

  \begin{proof}
    Note that a matrix $M$ being idempotent implies that $x^2-x\in\Ann(M)$, so $\mu_M | x^2-x$ whence $\mu_M\in\{ x,x-1,x^2-x\}$. Let $A,B$ be two idempotent $n\times n$ matrices.\\

    The forward direction is true in general, so it suffices to show the reverse direction. Suppose that $A$ and $B$ have the same rank $0\leq k\leq n$. If $k=0$, then $A=B=0$ whence $A,B$ are equal and thus similar, so suppose $k\neq 0$. By the rank-nullity theorem, $\dim(E_0(A)) = n - \rk(A) = n-k = n-\rk(B) = \dim(E_0(B))$. By idempotence, $\mu_A,\mu_B\in\{ x,x-1,x^2-x\}$. If either $\mu_A$ or $\mu_B$ is $x$, then the corresponding matrix is 0 whence $k=0$ contradicting that $k\neq 0$. Thus $\mu_A,\mu_B\in\{ x-1,x^2-x\}$. If either $\mu_A$ or $\mu_B$ is $x-1$ then the corresponding matrix is the identity whence it does not have 0 as an eigenvalue and thus $0=\dim(E_0(A))=\dim(E_0(B))$. Note that the other minimal polynomial cannot be $x^2-x$ as otherwise the eigenspace corresponding to zero would have positive dimension. Thus in this case both matrices are the identity and thus similar. Lastly, suppose that $\mu_A = x^2-x = x(x-1)= \mu_B$. Then all Jordan blocks in the JCF of $A$,$B$ have size 1. As $A,B$ have the same dimensions of their 0-eigenspaces, it follows that they have the same number of 0 blocks. But then they have the same number of 1 blocks as this is the only other eigenvalue and all blocks have size 1. Thus, they have the same JCF and are similar.
  \end{proof}
\end{homeworkProblem}


\begin{homeworkProblem}
  Let $F$ be an algebraically closed field and $V$ a finite dimensional $F$-vector space. \\


  \textbf{(a)} Let $S,T\in\mathcal{L}(V)$ such that $ST=TS$. Let $\lambda$ be an eigenvalue of $S$ and $E_\lambda(S)\leq V$ be the corresponding eigenspace of $S$. Prove that $E_\lambda(S)$ is a $T$-invariant subspace.

  \begin{proof}
    Let $v\in E_\lambda(S)$, so $Sv = \lambda v$. Then
    \[
      S(Tv) = (ST)(v) = (TS)(v) = T(Sv) = T(\lambda v) = \lambda\cdot (Tv)
    \]
    so $Tv\in E_\lambda(S)$. Thus $T(E_\lambda(S))\sub E_\lambda(S)$.
  \end{proof}

  \textbf{(b)} Assume that $T\in\mathcal{L}(V)$ is diagonalizable and let $W\leq V$ be a $T$-invariant subspace. Prove that $T|_W\in\mathcal{L}(W)$ is also diagonalizable.

  \begin{proof}
    Since $T$ is diagonalizable, $E_\lambda(T) = V_\lambda(T)$ for all $\lambda\in\Spec(T)$. Let $\lambda\in\Spec(T\vert_W)\sub\Spec(T)$ and $w\in W_\lambda(T\vert_W)$. Then, for some $k\in\N$, $(T-\lambda I)^k(w)=(T\vert_W - \lambda I\vert_W)^k(w) = 0$. Hence $w\in V_\lambda(T) = E_\lambda(T)$. But $w\in W$ so then $w\in E_\lambda(T\vert_W)$ whence $E_\lambda(T\vert_W) = W_\lambda(T\vert_W)$. So $T\vert_W$ is diagonalizable.
  \end{proof}

  \textbf{(c)} Assume again that $S,T\in\mathcal{L}(V)$ such that $ST=TS$. Prove that there exists a basis $\Omega$ of $V$ such that $[T]_\Omega$, and $[S]_\Omega$ are both diagonal.

  \begin{proof}
    I am quite sure that this claim is false as stated, so I will add the assumption that $S,T$ are both diagonalizable.\\

    Then as $S$ is diagonalizable,
    \[
      V = \bigoplus_{\lambda\in\Spec(S)}V_\lambda(S) = \bigoplus_{\lambda\in\Spec(S)} E_\lambda(S).
    \]

    Fix $\lambda\in \Spec(S)$. By part (a), $E_\lambda(S)$ is $T$-invariant whence part (b) implies that $T\vert_{E_\lambda(S)}$ is diagonalizable. So
    \[
      E_\lambda(S) = \bigoplus_{\delta\in\Spec(T\vert_{E_\lambda(S)})}E_\delta(T\vert_{E_\lambda(S)}).
    \]
    Now we write
    \[
      V = \bigoplus_{\lambda\in\Spec(S)} \bigoplus_{\delta\in\Spec(T\vert_{E_\lambda(S)})}E_\delta(T\vert_{E_\lambda(S)}).
    \]
    Since this sum is direct, we may form a basis for $V$ from bases for $E_\delta(T\vert_{E\lambda(S)})$ over this double direct sum, whence this basis is both an eigenbasis for $T$ and $S$.
  \end{proof}

  \textbf{(d)} Give an example of a vector space $V$ with $\dim_F(V)\geq 3$ and two commuting linear transformations $S,T\in\mathcal{L}(V)$ such that NO basis $\Omega$ of $V$ exists such that both $[T]_\Omega$, and $[S]_\Omega$ are in JCF.

  \begin{proof}
    Consider the matrices
    \[
      S = \begin{pmatrix}
        0&1&0\\0&0&0\\0&0&0
      \end{pmatrix} \hspace{20pt} T = \begin{pmatrix}
        0&0&0\\0&0&0\\0&1&0
      \end{pmatrix}
    \]
    Note that $ST$ and $TS$ are both the zero matrix so these matrices commute. Moreover, $S$ is already in JCF and the JCF of $T$ is precisely $S$. Thus, if we pick a basis bringing $T$ into JCF, we would end up knocking $S$ out of JCF and vice versa.
  \end{proof}

\end{homeworkProblem}


\begin{homeworkProblem}
  Find the number of distinct conjugacy classes in the group $GL_3(\Z/2\Z)$, and specify one element in each conjugacy class. \\

  \begin{proof}
    Fix $A\in \GL_3(\Z/2\Z)$. Let $\alpha_1|\cdots|\alpha_m = \mu_A$ be the invariant factors for $A$. Then $\deg(\alpha_1)+\cdots+\deg(\alpha_m) = 3$ and $\alpha_1\cdots\alpha_m = \chi_A$. As $\det(A)\neq 0$, it follows that $\det(A) = 1$ so $\chi_A(x) = x^3 + ax^2 + bx + 1$. \\

    \underline{$a=0,\ b=0$}: $\chi_A(x) = x^3+1 = (x+1)(x^2+x+1)$. Both $x+1$ and $x^2+x+1$ are irreducible over $\Z/2\Z$, so each $\alpha_i\in\{ x+1, x^2+x+1, x^3+1\}$. Note that $\alpha_m = x+1,x^2+x+1$ are both impossible as they are both not equal to the characteristic polynomial and thus irreducibility would force lower factors to equal $\alpha_m$ and thus would miss the other respective factor. So $\alpha_m = x^3+1$, whence $m=1$ and $A$ is similar to \[\begin{bmatrix} 0&0&1 \\ 1&0&0 \\ 0&1&0 \end{bmatrix} \]

    \underline{$a=1,\ b=0$}: $\chi_A(x) = x^3+x^2+1$. This polynomial is irreducible over $\Z_2$ and already degree 3, so $m=1$ and $A$ is similar to \[\begin{bmatrix} 0&0&1 \\ 1&0&0 \\ 0&1&1 \end{bmatrix}\]

    \underline{$a=0,\ b=1$}: $\chi_A(x) = x^3+x+1$. This polynomial is irreducible over $\Z_2$ and already degree 3, so $m=1$ and $A$ is similar to \[\begin{bmatrix} 0&0&1 \\ 1&0&1 \\ 0&1&0 \end{bmatrix}\]

    \underline{$a=1,\ b=1$}: $\chi_A(x) = x^3+x^2+x+1 = (x+1)^3$. In this case, via partitions of $3$ we have either $m=1$ so $\alpha_1 = \chi_A$, $m=2$ and $\alpha_1 = x+1$ and $\alpha_2 = (x+1)^2$, or $m=3$ and $\alpha_1= \alpha_2 = \alpha_3 = x+1$. Hence $A$ is similar to one of the following:
    \[
      \begin{bmatrix} 0&0&1 \\ 1&0&1 \\ 0&1&1 \end{bmatrix} \hspace{20pt}
      \begin{bmatrix} 1&0&0 \\ 0&0&1 \\ 0&1&0 \end{bmatrix} \hspace{20pt}
      \begin{bmatrix} 1&0&0 \\ 0&1&0 \\ 0&0&1 \end{bmatrix}
    \]
  \end{proof}
\end{homeworkProblem}


\begin{homeworkProblem}
  Let $V$ be an $n$-dimensional vector space over an algebraically closed field and $T\in\mathcal{L}(V)$. Assume that $T$ has just one eigenvalue $\lambda$ and just one Jordan block. Let $S=T-\lambda I$.\\

  \textbf{(a)} Prove that $\rk(S^k)=n-k$, for all $0\leq k\leq n$. Deduce that $\Im(S^k)=\ker(S^{n-k})$, for all $0\leq k\leq n$.

  \begin{proof}
    Note that $n_T(k,\lambda) = 1$ for $0\leq k\leq n$ by assumption.\\

    We induct on $0\leq k\leq n$. For $k=0$, $S^0 = I$ so $\rk(S^0) = n = n-0$.\\

    Now suppose $0<k\leq n$ and that the claim holds for $k-1$. On one hand, by the induction hypothesis $\rk(S^{k-1}) = n-(k-1)$. On the other hand
    \[
      1 = n_T(k,\lambda) = \rk((T-\lambda I)^{k-1}) - \rk((T-\lambda I)^{k}) = \rk(S^{k-1}) - \rk(S^k) = n - k + 1 - \rk(S^k) \implies \rk(S^k) = n-k.
    \]

    To see that $\Im(S^k) = \ker(S^{n-k})$, note that by the rank nullity theorem we have \[\dim\ker(S^{n-k}) = n - \rk(S^{n-k}) = n - (n-(n-k)) = n-k = \rk(S^k) = \dim\Im(S^k),\] so it suffices to show that $\Im(S^k)\sub\ker(S^{n-k})$. \\

    Take $w\in \Im(S^k)$. Then $w = S^k v$ for some $v\in V$. Noting that $\rk(S^n) = 0\implies S^n = \rm{O}$, we have that $S^{n-k}w = S^{n-k}(S^k v) = S^n v = \rm{O}v = 0$, so $w\in \ker(S^{n-k})$.
  \end{proof}

  \textbf{(b)} Let $v\in V$ be any vector which lies outside of $\Im(S)=\ker(S^{n-1})$. Prove that $\{S^{n-1}v,\ldots, Sv, v\}$ is a Jordan basis for $T$.

  \begin{proof}
    As $v\not\in\ker(S^{n-1})$, we have that $S^{n-1}v \neq 0$ whilst
    \[
      (T-\lambda I)S^{n-1}v = S^n v = \rm{O}v = 0 \implies T(S^{n-1}v) = \lambda\cdot(S^{n-1}v).
    \]
    If $0\leq k <n-1$,
    \[
      (T-\lambda I)S^k v = S(S^k v) = S^{k+1}v \implies T(S^k v) = \lambda S^k v + S^{k+1}v
    \]
    Hence, $\Omega = \{ S^{n-1}v,\ldots, Sv, v\}$ is a Jordan chain. For ease of notation, let $v_k = S^k v$ for $0\leq k\leq n-1$. Then for each $0\leq k\leq n-1$, $(T-\lambda I)^{n-k}v_k = 0$ and $(T-\lambda I)^{n-k-1}$ so $v_k\in \ker(T-\lambda I)^{n-k}\setminus \ker(T-\lambda I)^{n-(k+1)}$. Moreover, we have the series
    \[
      0 = \ker(T-\lambda I)^{n-n} \sub \ker(T-\lambda I)^{n-(n-1)} \sub \cdots \sub \ker(T-\lambda I)^{n-1}\sub \ker(T-\lambda I)^{n-0} = V_\lambda(T) = V,
    \]
    so at each step we add in a vector linearly independent from the previously added vectors, whence induction implies that $\Omega$ is a linearly independent set and thus a basis by dimensionality. As $\Omega$ is a Jordan chain, it follows that $[T]_{\Omega} = J(n,\lambda)$.
  \end{proof}
\end{homeworkProblem}


% \begin{homeworkProblem}
%   Assume again that $V$ is an $n$-dimensional vector space over an algebraically closed field $F$ and $T\in\mathcal{L}(V)$. \\
%
%   \textbf{(a)} Assume that $T$ has unique eigenvalue $0$ and two Jordan blocks: a $1\times 1$ block and a $2\times 2$ block (so $n=3$ in this case). Justify the following algorithm for computing a Jordan basis for $T$: Take any $v\in V\setminus \ker(T)$ and choose $w\in\ker(T)$ such that $\{w, Tv\}$ is a basis for $\ker(T)$ (why is this possible?); then $\{w, Tv, v\}$ is a Jordan basis for $T$. \\
%
%   \textbf{(b)} Assume that $T$ has unique eigenvalue $0$ and two Jordan blocks, both of which are $2\times 2$ (so $n=4$). State an algorithm for finding a Jordan basis similar to the one in (a). \\
%
%   \textbf{(c)} Assume that for each $\lambda\in\Spec(T)$ there is only one Jordan $\lambda$-block in $JCF(T)$. Describe an algorithm for computing a Jordan basis of $T$. \textbf{Hint:} You just need a minor generalization of the algorithm in the previous problem.
% \end{homeworkProblem}


\begin{homeworkProblem}[6]
  Compute the Jordan canonical form and a Jordan basis for each of the following matrices over $\Q$:

  \hspace{60pt}(a) $\left(\begin{array}{ccc}
  -1 & 3 & 0\\
  0 & 2 & 0 \\
  2 & 1 & -1
  \end{array}\right)$ \hspace{100 pt} (b)  $\left(\begin{array}{ccc}
  1 & -1 & 1\\
  1 & -1 & 1 \\
  1 & -1 & 0
  \end{array}\right)$.
  \\
  \\

  \textbf{(a)}:
  \begin{proof}
    First we compute the characteristic polynomial of $A$:
    \[
      \chi_A(x) = \det(xI-A) = (x-2)(x+1)^2
    \]
    so $\Spec(A) = \{ -1, 2\}$. Now we find the number of Jordan blocks corresponding to each eigenvalue via computing the corresponding eigenspaces and consequently their dimensions. \\

    Note that $E_{-1}(A) = \ker(A+I) = \spn\{\ilm{ 0\\0\\1 }\}$ and $E_{2}(A) = \ker(A-2I) = \ilm{1\\1\\1}\}$. As $\dim(E_{-1}(A)) = 1$ and the power of $x+1$ in $\chi_A$ is 2 (the sum of the sizes of the blocks), it  follows that $JCF(A)$ has one $-1$-block of size 2 and thus we are forced to have
    \[
      JCF(A) = \begin{pmatrix}
        -1 & 1 & 0 \\
        0 &-1 & 0\\
        0 & 0 & 2
      \end{pmatrix}.
    \]
    To find a Jordan basis, note that $\dim V_{-1}(A) = 2$ as it is the sum of the sizes of all the $-1$-blocks in $JCF(A)$. To obtain a Jordan cycle, we solve for a $w$ such that $\ilm{0\\0\\1}=(A+I)w$. We compute that $w = \ilm{1/2\\0\\0}$ works as a solution, whence we obtain an actual basis for $V_{-1}(A)$ by dimensionality. Hence, our Jordan basis is
    \[
      \left\{\ilm{1/2\\0\\0}, \ilm{ 0\\0\\1 }, \ilm{1\\1\\1}\right\}.
    \]

  \end{proof}

  \textbf{(b)}:

  \begin{proof}
    First we compute the characteristic polynomial of $B$:
    \[
      \chi_B(x) = \det\begin{pmatrix}
        x-1 & 1 & -1 \\ -1 & x+1 & -1 \\ -1 & 1 & x
      \end{pmatrix} = x^3
    \]
    so $\Spec(B) = \{ 0\}$. We now compute eigenspaces and generalized eigenspaces. We compute that $E_0(B) = \ker(B) = \spn\{ \ilm{1\\1\\0}\}$, so $\dim(E_0(B)) = 1$ whence there is one $0$-block and thus
    \[
      JCF(A) = \begin{pmatrix}
        0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0
      \end{pmatrix}
    \]
    Let $v_2 = \ilm{1\\1\\0}$. We seek nonzero $v_1,v_0\in V_0(B)$ such that $v_2 = A v_1$ and $v_1 = A v_0$. We compute that the choices $v_1 = \ilm{0\\0\\1}$ and $v_0 = \ilm{1\\0\\-1}$ possess the desired properties. Thus our Jordan basis is given by
    \[
      \left\{\ilm{1\\1\\0},\ \ilm{0\\0\\1},\ \ilm{1\\0\\-1}\right\}.
    \]
  \end{proof}
\end{homeworkProblem}

\begin{homeworkProblem}
  Let $F=\mathbb{F}_3$ be the field with $3$ elements and let $A\in M_{12}(\mathbb{F}_3)$. Suppose that $A$ satisfies all the following assumptions:
  \begin{itemize}
    \item $\rk(A)=10$, \hspace{20pt} $\rk(A^2)=9$, \hspace{20pt} $\rk(A^3)=9$.
    \item $\rk(A-I)=12$.
    \item $\rk(A-2I)=9$, \hspace{20pt} $\rk((A-2I)^2)=7$, \hspace{20pt} $\rk((A-2I)^3)=6$.
  \end{itemize}

  \textbf{(a)} Assume in addition that the characteristic polynomial $\chi_A(x)$ splits completely over $F$ (i.e. it splits into linear factors in $F[x]$). Find the Jordan canonical form of $A$.

  \begin{proof}
    For brevity, we write $n(k,\lambda) = n_A(k,\lambda)$. Recall that
    $n_A(k,\lambda) = \rk((A-\lambda I)^{k-1}) - \rk((A-\lambda I)^k)$ gives the number of Jordan blocks corresponding to $\lambda$ of size at least $k$. We compute,
    \begin{align*}
      n(1,0) &= \rk(I)-\rk(A) = 12-10 = 2 \\
      n(2,0) &= \rk(A)-\rk(A^2) = 10-9 = 1 \\
      n(3,0) &= \rk(A^2)-\rk(A^3) = 9-9 = 0 \\
    \end{align*}
    whence we have one 0-block of size 1, and one 0-block of size 2,
    \begin{align*}
      n(1,1) &= \rk(I) - \rk(A-I) = 12-12 = 0
    \end{align*}
    giving that 1 is not an eigenvalue of $A$, and
    \begin{align*}
      n(1,2) &= \rk(I) - \rk(A-2I) = 12-9 = 3 \\
      n(2,2) &= \rk(A-2I) - \rk((A-2I)^2) = 9-7 = 2 \\
      n(3,2) &= \rk((A-2I)^2) - \rk((A-2I)^3) = 7-6 = 1 \\
    \end{align*}
    implying that we have one 2-block of size 1, one 2-block of size 2, and the remaining space given by a single 2-block of size $12-(2+1+2+1) = 6$.
    \setcounter{MaxMatrixCols}{20}
    \begin{equation}
      \begin{pmatrix}
      2 & 1 & 0 & 0 & 0 & 0 &  &  &  &  &  &  \\
      0 & 2 & 1 & 0 & 0 & 0 &  &  &  &  &  &  \\
      0 & 0 & 2 & 1 & 0 & 0 &  &  &  &  &  &  \\
      0 & 0 & 0 & 2 & 1 & 0 &  &  &  &  &  &  \\
      0 & 0 & 0 & 0 & 2 & 1 &  &  &  &  &  &  \\
      0 & 0 & 0 & 0 & 0 & 2 &  &  &  &  &  &  \\
       &  &  &  &  &  & 2 & 1 &  &  &  &  \\
       &  &  &  &  &  & 0 & 2 &  &  &  &  \\
       &  &  &  &  &  &  &  & 2 &  &  &  \\
       &  &  &  &  &  &  &  &  & 0 & 1 &  \\
       &  &  &  &  &  &  &  &  &  & 0 &  \\
       &  &  &  &  &  &  &  &  &  &  & 0
     \end{pmatrix}
    \end{equation}
  \end{proof}

  \textbf{(b)} Find all possible RCF's of matrices $A$ satisfying all the bullet assumptions, but not necessarily the extra assumption in (a).

  \begin{proof}
    % Embedding $F$ inside its algebraic closure, we have that $RCF_{\cls{F}}(A) = RCF_{F}(A)$ and $RCF_{\cls{F}}(A)$ is similar to $JCF_{\cls{F}}(A)$ over $\cls{F}$. However, $B:=JCF_{\cls{F}}(A)$ is given above and is in fact inside $M_{12}(F)$, so $RCF_{F}(A)$ is similar to $B$ over $F$. Thus $\chi_A = \chi_{RCF(A)} = \chi_B = x^3(x-2)^9$ and $\mu_A = \mu_B = x^2 (x-2)^6$. Let $\alpha_1|\cdots|\alpha_m$ be the invariant factors for $A$. Then
    % \[
    %   \alpha_1\cdots \alpha_{m-1} = \frac{x^3(x-2)^9}{x^2(x-2)^6} = x(x-2)^3
    % \]
    % Via the successive divisbility restriction, one factor of $x$ must appear in $\alpha_{m-1}$ but not in any lower factors. Moreover, we must have that $\alpha_{m-1}$ has at least one factor of $x-2$ as otherwise the product would not have a factor of $x-2$. Thus we may write $\alpha_{m-1} = x(x-2)^k$ with $1\leq k\leq 3$.\\
    %
    % \underline{$k=1$}: $\alpha_{m-1} = x(x-2)$. Successive divisibility forces $m=4$ and $\alpha_1 = \alpha_2 = x-2$.\\
    %
    % \underline{$k=2$}: $\alpha_{m-1} = x(x-2)^2$. Degree constraints force $m=3$ and $\alpha_1 = x-2$.\\
    %
    % \underline{$k=3$}: $\alpha_{m-1} = x(x-2)^3$. Then we have $m=2$ so $\alpha_1 = x(x-2)^3$ by assumption.
    Write $V = \F_3^{12}$. Then, writing $V_A$ in invariant factor form:
    \[
      V_A \cong \frac{F[x]}{(\alpha_1(x))} \oplus \frac{F[x]}{(\alpha_2(x))} \oplus\cdots\oplus \frac{F[x]}{(\alpha_m(x))}
    \]
    In elementary divisor form, this becomes
    \[
      V_A \cong \frac{F[x]}{(x)} \oplus \frac{F[x]}{(x^2)} \oplus \frac{F[x]}{((x-2))} \oplus \frac{F[x]}{((x-2)^2)} \oplus \frac{F[x]}{((x-2)^k)} \oplus \frac{F[x]}{(p(x))}
    \]
    where $p(x)\in\F_3[x]$, $k\geq3$, and there is no factor corresponding to $x-1$ as $\rk(A-I) = 12$ implies that $1$ is not an eigenvalue of $A$. Note that $\mu_A = x^2(x-2)^k p(x)$. As $12 = \sum \deg(\alpha_i) = 1 +2 +1 +2 +k +\deg(p) \implies k = 6-\deg(p) \leq 6$.\\

    \underline{$k=3$}: Then $\deg(p) = 6-3 = 3$, whence $p$ must be irreducible as otherwise it would have a linear fact and thus contribute an eigenspace which we have already filled by rank restrictions. Hence, the RCFs appear in corresponding partitions of the factors $x^2(x-2)^3$ into lower $\alpha_i$'s with successive division and degrees summing to 12.\\

    \underline{$k=4$}: The $\deg(p) = 6-4 = 2$, whence $p$ must be irreducible by the above logic. Hence, the RCFs appear in corresponding partitions of the factors $x^2(x-2)^4$ into lower $\alpha_i$'s with successive division and degrees summing to 12.\\

    \underline{$k=5$}: Then $\deg(p) = 1$, which is impossible as then we would get another addition to the already filled eigenspace.\\

    \underline{$k=6$}: Then $\deg(p) = 0$, so $p = 1$. The RCFs appear then in corresponding partitions of the factors $x^2(x-2)^6$ in the lower $\alpha_i$'s to obtain total degree sum of 12.
  \end{proof}

\end{homeworkProblem}

\end{document}
